{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b522e4cd",
   "metadata": {},
   "source": [
    "Install depedencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c87c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install unsloth\n",
    "# Also get the latest nightly Unsloth!\n",
    "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0662fa13",
   "metadata": {},
   "source": [
    "Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25e5662",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"model_name\": \"unsloth/Qwen2.5-0.5B-Instruct-bnb-4bit\",\n",
    "    #\"model_name\": \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\",\n",
    "    #\"model_name\": \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n",
    "    \"max_seq_length\": 128,\n",
    "    \"batch_size\": 128,\n",
    "    \"valid_ratio\": 0.2,\n",
    "    \"dataset_path\": \"datasets/5k_bgl_train.txt\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4aa56e7",
   "metadata": {},
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4400e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"text\", data_files=config[\"dataset_path\"])\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=config[\"valid_ratio\"], shuffle=False, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bce21d0",
   "metadata": {},
   "source": [
    "Creating of the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fdfa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "from tokenizers import Tokenizer, models, normalizers, pre_tokenizers, processors\n",
    "\n",
    "all_logkeys = set()\n",
    "with open(config[\"dataset_path\"], \"r\") as f:\n",
    "    for line in f:\n",
    "        logkeys = line.strip().split()\n",
    "        all_logkeys.update(logkeys)\n",
    "\n",
    "print(f\"Found {len(all_logkeys)} unique log keys\")\n",
    "\n",
    "special_tokens = [\"<bos>\", \"<eos>\", \"<pad>\", \"<unk>\"]\n",
    "vocab = special_tokens + list(all_logkeys)\n",
    "\n",
    "vocab_dict = {token: i for i, token in enumerate(vocab)}\n",
    "\n",
    "raw_tokenizer = Tokenizer(models.WordLevel(vocab=vocab_dict, unk_token=\"<unk>\"))\n",
    "\n",
    "raw_tokenizer.pre_tokenizer = pre_tokenizers.WhitespaceSplit()\n",
    "\n",
    "raw_tokenizer.normalizer = normalizers.Sequence([])\n",
    "\n",
    "raw_tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"<bos> $A <eos>\",\n",
    "    special_tokens=[(\"<bos>\", vocab_dict[\"<bos>\"]), (\"<eos>\", vocab_dict[\"<eos>\"])],\n",
    ")\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=raw_tokenizer,\n",
    "    bos_token=\"<bos>\",\n",
    "    eos_token=\"<eos>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    model_max_length=config[\"max_seq_length\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0b7c59",
   "metadata": {},
   "source": [
    "Tokenization of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7630217",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_tokenize(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=config[\"max_seq_length\"])\n",
    "\n",
    "\n",
    "valid_tokenized_dataset = dataset[\"test\"].map(\n",
    "    sliding_window_tokenize,\n",
    "    batched=True,\n",
    "    batch_size = config[\"batch_size\"],\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_tokenized_dataset = dataset[\"train\"].map(\n",
    "    sliding_window_tokenize,\n",
    "    batched=True,\n",
    "    batch_size = config[\"batch_size\"],\n",
    "    remove_columns=[\"text\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533403c8",
   "metadata": {},
   "source": [
    "Loading of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bc1c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, seed=42)\n",
    "\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from unsloth import FastModel\n",
    "import torch\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from peft import LoraConfig\n",
    "\n",
    "\n",
    "model, _ = FastModel.from_pretrained(\n",
    "    config[\"model_name\"],\n",
    "    max_seq_length=config[\"max_seq_length\"],\n",
    "    load_in_4bit=True,\n",
    "    device_map=\"auto\",\n",
    "    dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    finetune_language_layers   = True,  # Should leave on!\n",
    "    finetune_mlp_modules       = True,  # SHould leave on always!\n",
    "    inference_mode=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1901e2",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a682e5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=3)\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_tokenized_dataset,\n",
    "    eval_dataset=valid_tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[early_stopping_callback],\n",
    "    args=SFTConfig(\n",
    "      output_dir=\"./output\",\n",
    "      eval_strategy=\"epoch\",\n",
    "      save_strategy=\"epoch\",\n",
    "      learning_rate=2e-4,\n",
    "      num_train_epochs=200,\n",
    "      per_device_train_batch_size=3072,\n",
    "      per_device_eval_batch_size=3072,\n",
    "      metric_for_best_model=\"eval_loss\",\n",
    "      bf16=True,\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3fffd5",
   "metadata": {},
   "source": [
    "Saving the model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336f73e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "best_model_source = str(trainer.state.best_model_checkpoint)\n",
    "destination = \"drive/MyDrive/final\" #change this to the name you wanto to save the model\n",
    "\n",
    "# Check if the best_model directory exists\n",
    "if os.path.exists(best_model_source):\n",
    "    try:\n",
    "        shutil.copytree(best_model_source, destination, dirs_exist_ok=True)\n",
    "        print(f\"Best model saved to: {destination}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error copying best model: {e}\")\n",
    "else:\n",
    "    print(\"Best model directory not found. Ensure training completed and best model saving was configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d5f6b4",
   "metadata": {},
   "source": [
    "Load the trained model to for calculating the K "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2a752b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastModel\n",
    "from transformers import AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"drive/MyDrive/final\") #change this to where the model was saved\n",
    "\n",
    "model, _ = FastModel.from_pretrained(\n",
    "    \"drive/MyDrive/final\", #change this to where the model was saved\n",
    "    max_seq_length=config[\"max_seq_length\"],\n",
    "    load_in_4bit=True,\n",
    "    resize_model_vocab=len(tokenizer),\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model = FastModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1777ae",
   "metadata": {},
   "source": [
    "Get k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ac8fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "def calculate_multiple_topk_miss_rates(sequence, topk_candidates):\n",
    "    model.eval()\n",
    "    device = model.device\n",
    "    topk_candidates = sorted(set(topk_candidates))\n",
    "\n",
    "    print(f\"Starting evaluation with top_k values: {topk_candidates} on device: {device}\")\n",
    "\n",
    "    miss_rates_by_k = {k: [] for k in topk_candidates}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=len(sequence), desc=f\"Evaluation\", unit=\"sequence\") as pbar:\n",
    "            for seq_idx, token_list in enumerate(sequence):\n",
    "                token_tensor = torch.tensor(token_list, dtype=torch.long, device=device).unsqueeze(0)\n",
    "                input_ids = token_tensor[:, :-1]\n",
    "                labels = token_tensor[:, 1:]\n",
    "\n",
    "                if input_ids.size(1) == 0:\n",
    "                    for k in topk_candidates:\n",
    "                        miss_rates_by_k[k].append(1.0)\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "\n",
    "                with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                    outputs = model(input_ids=input_ids)\n",
    "\n",
    "                logits = outputs.logits.to(torch.float32)\n",
    "\n",
    "                unk_token_id = tokenizer.convert_tokens_to_ids(\"<unk>\")\n",
    "                if unk_token_id is not None and 0 <= unk_token_id < logits.shape[-1]:\n",
    "                    logits[:, :, unk_token_id] = float('-inf')\n",
    "\n",
    "                total_tokens_to_predict = labels.size(1)\n",
    "\n",
    "                if total_tokens_to_predict <= 2:\n",
    "                    for k in topk_candidates:\n",
    "                        miss_rates_by_k[k].append(0.0)\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "\n",
    "                max_k = max(topk_candidates)\n",
    "                _, topk_predictions = torch.topk(logits, k=max_k, dim=-1)\n",
    "\n",
    "                for k in topk_candidates:\n",
    "                    correct = 0\n",
    "                    for i in range(total_tokens_to_predict):\n",
    "                        true_token_id = labels[0, i].item()\n",
    "                        predicted_k_token_ids = topk_predictions[0, i, :k].tolist()\n",
    "                        if true_token_id in predicted_k_token_ids:\n",
    "                            correct += 1\n",
    "                    miss_rate = 1.0 - (correct / total_tokens_to_predict)\n",
    "                    miss_rate = 0.01 if (0 < miss_rate < 0.01) else miss_rate\n",
    "                    miss_rates_by_k[k].append(miss_rate)\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "    return {k: np.array(miss_rates) for k, miss_rates in miss_rates_by_k.items()}\n",
    "\n",
    "tokenizer_vocab_size = len(tokenizer)\n",
    "topk_candidates = list(range(1, tokenizer_vocab_size + 1))\n",
    "miss_rates_dict = calculate_multiple_topk_miss_rates(valid_tokenized_dataset[\"input_ids\"], topk_candidates)\n",
    "\n",
    "for k, miss_rates in miss_rates_dict.items():\n",
    "    print(f\"Top-{k} avg miss rate: {miss_rates.mean():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
