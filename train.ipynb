{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b522e4cd",
   "metadata": {},
   "source": [
    "Install depedencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c87c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install unsloth\n",
    "# Also get the latest nightly Unsloth!\n",
    "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0662fa13",
   "metadata": {},
   "source": [
    "Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25e5662",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    #\"model_name\": \"unsloth/Qwen2.5-0.5B-Instruct-bnb-4bit\",\n",
    "    #\"model_name\": \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\",\n",
    "    \"model_name\": \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n",
    "    \"max_seq_length\": 128,\n",
    "    \"batch_size\": 128,\n",
    "    \"valid_ratio\": 0.2,\n",
    "    \"dataset_path\": \"datasets/5k_bgl_train.txt\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4aa56e7",
   "metadata": {},
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4400e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"text\", data_files=config[\"dataset_path\"])\n",
    "print(dataset)\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=config[\"valid_ratio\"], shuffle=False, seed=42)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bce21d0",
   "metadata": {},
   "source": [
    "Creating of the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fdfa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "from tokenizers import Tokenizer, models, normalizers, pre_tokenizers, processors\n",
    "\n",
    "all_logkeys = set()\n",
    "with open(config[\"dataset_path\"], \"r\") as f:\n",
    "    for line in f:\n",
    "        logkeys = line.strip().split()\n",
    "        all_logkeys.update(logkeys)\n",
    "\n",
    "print(f\"Found {len(all_logkeys)} unique log keys\")\n",
    "\n",
    "\n",
    "special_tokens = [\"<bos>\", \"<eos>\", \"<pad>\", \"<unk>\"]\n",
    "vocab = special_tokens + list(all_logkeys)\n",
    "\n",
    "vocab_dict = {token: i for i, token in enumerate(vocab)}\n",
    "\n",
    "raw_tokenizer = Tokenizer(models.WordLevel(vocab=vocab_dict, unk_token=\"<unk>\"))\n",
    "\n",
    "raw_tokenizer.pre_tokenizer = pre_tokenizers.WhitespaceSplit()\n",
    "\n",
    "raw_tokenizer.normalizer = normalizers.Sequence([])\n",
    "\n",
    "raw_tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"<bos> $A <eos>\",\n",
    "    special_tokens=[(\"<bos>\", vocab_dict[\"<bos>\"]), (\"<eos>\", vocab_dict[\"<eos>\"])],\n",
    ")\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=raw_tokenizer,\n",
    "    bos_token=\"<bos>\",\n",
    "    eos_token=\"<eos>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    model_max_length=config[\"max_seq_length\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0b7c59",
   "metadata": {},
   "source": [
    "Tokenization of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7630217",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_tokenize(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=config[\"max_seq_length\"])\n",
    "\n",
    "valid_tokenized_dataset = dataset[\"test\"].map(\n",
    "    sliding_window_tokenize,\n",
    "    batched=True,\n",
    "    batch_size = config[\"batch_size\"],\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_tokenized_dataset = dataset[\"train\"].map(\n",
    "    sliding_window_tokenize,\n",
    "    batched=True,\n",
    "    batch_size = config[\"batch_size\"],\n",
    "    remove_columns=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533403c8",
   "metadata": {},
   "source": [
    "Loading of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bc1c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "from unsloth import FastModel\n",
    "import torch\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from peft import LoraConfig\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, seed=42)\n",
    "\n",
    "\n",
    "model, _ = FastModel.from_pretrained(\n",
    "    config[\"model_name\"],\n",
    "    max_seq_length=config[\"max_seq_length\"],\n",
    "    load_in_4bit=True,\n",
    "    device_map=\"auto\",\n",
    "    dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    finetune_language_layers   = True,\n",
    "    finetune_mlp_modules       = True,\n",
    "    inference_mode=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1901e2",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a682e5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=3)\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_tokenized_dataset,\n",
    "    eval_dataset=valid_tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[early_stopping_callback],\n",
    "    args=SFTConfig(\n",
    "      output_dir=\"./output\",\n",
    "      eval_strategy=\"epoch\",\n",
    "      save_strategy=\"epoch\",\n",
    "      learning_rate=2e-4,\n",
    "      num_train_epochs=100,\n",
    "      per_device_train_batch_size=1028,\n",
    "      per_device_eval_batch_size=1028,\n",
    "      metric_for_best_model=\"eval_loss\",\n",
    "      bf16=True,\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3fffd5",
   "metadata": {},
   "source": [
    "Saving the model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336f73e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"trained_models/model\") #change the path where you want to save the trained model\n",
    "tokenizer.save_pretrained(\"tokenizers/tokenizer\") #change the path where you want to save the tokenizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
